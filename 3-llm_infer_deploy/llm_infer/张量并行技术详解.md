## 摘要

为了训练更大的 transformer 模型，作者引入了一种简单高效的 `intra-layer model parallel`（层内模型并行，也叫张量并行）方法，这种方法不需要安装的新的编译器和库，它和 pipeline model parallelism 互为正交且互补，并且只需在 native PyTorch 中插入少量 `communication operations` 就能完全实现。

## 介绍

随着模型参数量规模的不断扩大（模型参数内存），它们往往会超出现代处理器的内存承载能力，这就需要采用额外的内存管理策略，比如 `activation checkpointing` (Chen et al., 2016) 技术。另外常，用的优化算法 `Adam` 需要为每个参数分配额外内存来存储动量及其他状态信息（即优化器内存），这也限制了能够高效训练的模型规模。为了解决这一问题，模型并行技术通过对模型进行分割，使得各部分权重和对应的优化状态不必同时加载到同一处理器上，从而突破内存瓶颈。例如，GPipe (Huang et al., 2018) 和 Mesh-Tensorflow (Shazeer et al., 2018) 分别为不同类型的模型并行提供了框架，但它们**通常需要重写模型结构代码**，并依赖仍在不断完善的自定义编译器和框架。

由此，论文作者在本论文研究中提出了一种基于 `intra-layer model-parallelism` 的简单高效的模型并行方案。直接基于 transformer 的语言模型的固有结构（即不改变模型本身结构），开发了一种在 PyTorch 中高效训练的简单模型并行实现，无需自定义 C++ 代码或编译器。这种方案与 GPipe (Huang et al., 2018) 等方法所倡导的 pipeline-based model parallelism 相互独立、互补，可以为大规模模型训练提供更多灵活性。

## 参考资料

- [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053)
- [图解大模型训练之：张量模型并行(TP)，Megatron-LM](https://zhuanlan.zhihu.com/p/622212228)